{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Integrated Retail Analytics for Store Optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual  (Tanuj kumar)\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Machine Learning Capstone Project aimed to analyze and predict weekly sales of a retail store chain using real-world data. The goal was to build a predictive model that helps the business make data-driven decisions to maximize revenue, especially around holidays and promotions.\n",
        "\n",
        "Data Understanding and Preparation\n",
        "The dataset consisted of features like Weekly_Sales, Temperature, Fuel_Price, various MarkDown promotions, CPI, Unemployment, IsHoliday, and Type of store. The first step involved exploring the dataset, handling missing values, encoding categorical features, and treating outliers:\n",
        "\n",
        "Missing Values:\n",
        "\n",
        "MarkDown1-5: Filled with 0 assuming no promotion.\n",
        "\n",
        "CPI and Unemployment: Imputed using median.\n",
        "\n",
        "Type: Filled using mode (most frequent store type).\n",
        "\n",
        "Categorical Encoding:\n",
        "One-hot encoding was applied to Type to convert it into numerical format while avoiding multicollinearity (using drop_first=True).\n",
        "\n",
        "Outlier Treatment:\n",
        "IQR-based capping was applied to the Weekly_Sales column to reduce the influence of extreme values.\n",
        "\n",
        "\n",
        "Exploratory Data Analysis\n",
        "Visualization techniques like pair plots and box plots helped in identifying correlations between features. From the pair plot, we found moderate relationships between Weekly_Sales and variables like MarkDown1 and Unemployment, suggesting these factors play a role in influencing sales.\n",
        "\n",
        "\n",
        "\n",
        "Statistical Testing\n",
        "Two hypothesis tests were performed:\n",
        "\n",
        "Welch’s t-test was used to compare Weekly_Sales during holiday vs non-holiday weeks. The test showed a statistically significant difference, confirming that holidays impact sales.\n",
        "\n",
        "One-way ANOVA was applied to test differences in Weekly_Sales across store types (A, B, and C). The result was also statistically significant, implying that store type affects sales volume.\n",
        "\n",
        "Model Building and Evaluation\n",
        "Several ML models were built, including Linear Regression, Decision Tree, and Support Vector Regression (SVR). Hyperparameter tuning was performed using GridSearchCV for SVR to optimize parameters like kernel, C, epsilon, and degree.\n",
        "\n",
        "Evaluation Metrics:\n",
        "Mean Squared Error (MSE) and R² Score were used. R² was chosen as the primary metric for its ability to show how well the model explains variance, which is vital for business forecasting.\n",
        "\n",
        "Best Model:\n",
        "The tuned SVR model performed the best, offering improved R² and reduced MSE after hyperparameter optimization.\n",
        "\n",
        "Model Explainability\n",
        "SHAP (SHapley Additive exPlanations) or feature importance from tree-based models was used to interpret results. Features like MarkDown1, IsHoliday, and Unemployment were found to significantly influence predictions, aligning with domain knowledge.\n",
        "\n",
        "\n",
        "Model Deployment\n",
        "The best-performing SVR model was saved using joblib and reloaded to successfully predict on unseen data, confirming deployment readiness.\n",
        "\n",
        "\n",
        "Conclusion\n",
        "This project successfully developed a machine learning model that predicts weekly sales with reasonable accuracy. Through rigorous data cleaning, hypothesis testing, model tuning, and explainability tools, we gained insights into the factors influencing sales. The final model can help stakeholders optimize inventory, plan promotions, and make strategic decisions, especially around holidays and markdown periods—contributing to a positive business impact."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Tanujkumarsingh/Integrated-Retail-Analytics-for-Store-Optimization"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retail stores often struggle with identifying inefficiencies in operations and understanding customer behavior. This project aims to integrate multiple data sources within a retail environment to analyze store performance, identify bottlenecks, and suggest actionable insights for optimization."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Load CSVs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_sales = pd.read_csv('/content/drive/MyDrive/sales-data-set.csv')\n",
        "df_products = pd.read_csv('/content/drive/MyDrive/Features-data-set.csv')\n",
        "df_store = pd.read_csv('/content/drive/MyDrive/stores-data-set.csv')\n",
        "\n",
        "# Merge\n",
        "df = df_sales.merge(df_products, on=[\"Store\", \"Date\"]).merge(df_store, on=\"Store\")\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing value\n",
        "# Set up the figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "\n",
        "plt.title(\"Heatmap of Missing Values\")\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Rows\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Features Dataset\n",
        "Contains information about stores over time.\n",
        "\n",
        " 2. Stores Dataset\n",
        "Contains metadata about each store.\n",
        "\n",
        "3. Sales Dataset\n",
        "Contains weekly sales data for each department in each store."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Features Dataset\n",
        "Store: Unique identifier for each store (integer).\n",
        "\n",
        "Date: The specific week for which data is recorded (date format).\n",
        "\n",
        "Temperature: The average temperature during that week in the region (in Fahrenheit).\n",
        "\n",
        "Fuel_Price: The cost of fuel in the region during that week (in USD).\n",
        "\n",
        "MarkDown1 to MarkDown5: Promotional markdowns related to specific campaigns (may contain missing values).\n",
        "\n",
        "CPI: Consumer Price Index for the area — reflects the cost of goods and services.\n",
        "\n",
        "Unemployment: Unemployment rate in the store’s region (percentage).\n",
        "\n",
        "IsHoliday: Boolean value indicating whether the week includes a holiday (TRUE or FALSE).\n",
        "\n",
        "2. Stores Dataset\n",
        "Store: Unique identifier for each store (matches with Features and Sales datasets).\n",
        "\n",
        "Type: Type of store (categorical — e.g., A, B, or C).\n",
        "\n",
        "Size: Physical size of the store (in square feet).\n",
        "\n",
        "3. Sales Dataset\n",
        "Store: Unique store identifier (same as above).\n",
        "\n",
        "Dept: Department number within the store.\n",
        "\n",
        "Date: The specific week the sales occurred (same as in Features).\n",
        "\n",
        "Weekly_Sales: Total sales for that department in the given week (in USD).\n",
        "\n",
        "IsHoliday: Boolean value indicating if the sales week was during a holiday (TRUE or FALSE)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values for {column}: {unique_values}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "sales = pd.read_csv('/content/drive/MyDrive/sales-data-set.csv')\n",
        "features = pd.read_csv('/content/drive/MyDrive/Features-data-set.csv')\n",
        "stores = pd.read_csv('/content/drive/MyDrive/stores-data-set.csv')\n",
        "\n",
        "# Merge datasets\n",
        "df = pd.merge(sales, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "\n",
        "# Convert 'Date' to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
        "\n",
        "# Fill missing values in MarkDown columns with 0\n",
        "mark_down_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "df[mark_down_cols] = df[mark_down_cols].fillna(0)\n",
        "\n",
        "# Fill any remaining missing numeric values with the column mean\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "if 'Type' in df.columns:\n",
        "    df['Type'] = le.fit_transform(df['Type'])\n",
        "\n",
        "# (Optional) sort by date if needed\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "\n",
        "# Preview the wrangled dataset\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Manipulations Done\n",
        "1. Label Encoding\n",
        "2. Date Conversion\n",
        "3. Missing Value Handling\n",
        "4. Initial Merging of Datasets\n",
        "\n",
        "# Initial Insights You Might Expect\n",
        "1. Effect of Holidays on Sales\n",
        "2. Sales Trend by Store Type\n",
        "3. Impact of Fuel Price / CPI / Unemployment\n",
        "4. Markdown Promotions\n",
        "5. Temperature & Sales"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Barplot: Total Weekly Sales by Store\n",
        "sns.barplot(x='Store', y='Weekly_Sales', data=df, estimator=sum)\n",
        "plt.title('Total Sales by Store')\n",
        "plt.xlabel('Store')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason for choosing a bar chart:\n",
        "\n",
        "A bar chart is ideal for comparing total values across distinct categories (in this case, stores).\n",
        "\n",
        "It clearly visualizes which stores have higher or lower sales, helping identify performance variations.\n",
        "\n",
        "It's simple, readable, and gives a quick overview of sales distribution across all stores.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Insights from the chart:\n",
        "\n",
        "Some stores (like Store 4, 10, 14, 20) have consistently higher total sales, approaching or exceeding 300 million.\n",
        "\n",
        "Other stores (like Store 5, 36, 44) show significantly lower sales, even below 100 million.\n",
        "\n",
        "There's a wide disparity in performance, suggesting variability in store effectiveness, location impact, or local demand.\n",
        "\n",
        "Some stores (e.g., Store 35, Store 5) have much lower bars, which could indicate operational inefficiencies, smaller size, poor location, or underperformance."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Yes, here’s how:\n",
        "\n",
        "High-performing stores can be benchmarked to identify what they're doing right (e.g., promotions, layout, location, staffing).\n",
        "\n",
        "Low-performing stores can be flagged for deeper investigation — marketing support, inventory checks, or possibly restructuring.\n",
        "\n",
        "This helps in resource allocation, regional planning, and strategy formulation (e.g., which stores to expand or optimize)."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# For store type\n",
        "sns.barplot(x='Type', y='Weekly_Sales', data=df, estimator=sum)\n",
        "plt.title('Total Sales by Store Type')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are perfect for comparing discrete categories — in this case, Store Types (0, 1, 2).\n",
        "\n",
        "It makes it visually simple to compare total sales performance across different store formats or business models.\n",
        "\n",
        "The chart conveys differences clearly, even if the units are large (in billions)."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store Type 0 dominates in sales, generating well over 4 billion in total sales.\n",
        "\n",
        "Store Type 1 has less than half the sales of Type 0.\n",
        "\n",
        "Store Type 2 significantly underperforms, contributing only a small fraction of total sales.\n",
        "\n",
        " Interpretation:\n",
        "\n",
        "Store Type 0 could represent large-format stores (e.g., supercenters or regional hubs).\n",
        "\n",
        "Type 2, with very low total sales, might be small-format or specialized stores (e.g., neighborhood or express outlets).\n",
        "\n",
        "There’s a clear correlation between store type and revenue scal"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Yes, and here’s how:\n",
        "\n",
        "Positive business impact:\n",
        "\n",
        "Focus investment and expansion efforts on Type 0 stores, as they have the highest revenue-generating potential.\n",
        "\n",
        "Consider optimizing or re-evaluating the purpose and placement of Type 2 stores — they may need different strategies (e.g., targeted promotions, reduced operating costs).\n",
        "\n",
        "Use performance trends to tailor marketing, inventory, and staffing by store type."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# For department\n",
        "sns.barplot(x='Dept', y='Weekly_Sales', data=df, estimator=sum)\n",
        "plt.title('Total Sales by Department')\n",
        "plt.xlabel('Department')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason for choosing a bar chart:\n",
        "\n",
        "A bar chart is ideal for comparing individual department performances.\n",
        "\n",
        "It helps visualize total sales distribution across all departments in one frame.\n",
        "\n",
        "Especially useful when departments are discrete categorical variables and when evaluating sales concentration or variability."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a high variation in total sales across departments.\n",
        "\n",
        "A few departments (e.g., those toward the right and center) show very high total sales, reaching up to 5×10⁸.\n",
        "\n",
        "Some departments have consistently low total sales, indicating either less demand, smaller product assortment, or operational issues"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Yes, absolutely. Here's how:\n",
        "\n",
        "Positive business impact:\n",
        "\n",
        "High-performing departments can be leveraged more: expand their product range, allocate more space, or use them for cross-promotions.\n",
        "\n",
        "Underperforming departments offer chances for strategy revamp — adjust pricing, marketing focus, or shelf space.\n",
        "\n",
        "The store can allocate resources efficiently by understanding which departments drive the most revenue."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Store-wise Sales Distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(x='Type', y='Weekly_Sales', data=df)\n",
        "plt.title(\"Sales by Store Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot allows us to visually compare distributions, medians, and outliers of Weekly_Sales across different store types (0, 1, 2). It’s ideal to identify if some store types consistently perform better or worse."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type 1 stores show the highest outliers, indicating some weeks with extremely high sales.\n",
        "\n",
        "The median sales are fairly similar across types, but the spread (variance) is higher in Type 1.\n",
        "\n",
        "Type 2 stores have smaller sales overall, with fewer outliers."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nowing that Type 1 stores drive high peak sales can help:\n",
        "\n",
        "Focus promotions and inventory buildup in Type 1 stores.\n",
        "\n",
        "Investigate low performance or improve strategy for Type 2 stores.}"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Correlation\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is essential to examine linear relationships between numerical features. It highlights which variables are good candidates for predictive modeling or multicollinearity."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weekly_Sales has slight positive correlation with MarkDown1 and MarkDown5.\n",
        "\n",
        "MarkDown1 and MarkDown4 are highly correlated (0.84) — likely launched together.\n",
        "\n",
        "Unemployment and CPI are negatively correlated with Type.}\n",
        "}"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes.\n",
        "\n",
        "Strong inter-variable relationships help in feature selection and campaign bundling.\n",
        "\n",
        "Correlated markdowns suggest coordinated promotions could be more effective.\n",
        "\n",
        "Helps avoid multicollinearity in modeling."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Store-wise sales\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(x='Store', y='Weekly_Sales', data=df)\n",
        "plt.title(\"Sales Distribution by Store\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare sales performance across all individual stores. Box plots allow easy detection of outliers, store consistency, and central sales tendencies."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores like Store 13, 20, and 38 have frequent high sales outliers.\n",
        "\n",
        "Some stores (e.g., Store 33 or 45) consistently have low or flat distributions.\n",
        "\n",
        "There’s considerable variation in performance between stores."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes.\n",
        "\n",
        "Helps rank stores by performance.\n",
        "\n",
        "Poor performers can be re-evaluated for closure, expansion, or assistance.\n",
        "\n",
        "High performers can be studied to replicate success elsewhere."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns for correlation\n",
        "numeric_cols = [\n",
        "    'Weekly_Sales', 'Temperature', 'Fuel_Price',\n",
        "    'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "    'CPI', 'Unemployment'\n",
        "]\n",
        "\n",
        "# Drop rows with missing values\n",
        "heatmap_data = df[numeric_cols].dropna()\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = heatmap_data.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Chart 14: Correlation Heatmap of Numeric Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cleaned-up, focused heatmap (Chart 14) zooms in on numeric relationships, especially to understand sales influencers directly, removing redundant fields."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weekly_Sales shows a mild correlation with MarkDown1 (0.05) and others.\n",
        "\n",
        "MarkDown1 and MarkDown4 again are highly correlated → indicate co-marketing or same campaign source.\n",
        "\n",
        "Temperature, CPI, and Unemployment have very low influence on weekly sales."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Pair Plot visualization code\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Select relevant numerical columns\n",
        "# pairplot_data = df[[\n",
        "#     'Weekly_Sales', 'Temperature', 'Fuel_Price',\n",
        "#     'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "#     'CPI', 'Unemployment'\n",
        "# ]]\n",
        "\n",
        "# # Drop rows with missing values for clean plotting\n",
        "# pairplot_data = pairplot_data.dropna()\n",
        "\n",
        "# # Generate the pair plot\n",
        "# sns.pairplot(pairplot_data)\n",
        "# plt.suptitle(\"Chart 15: Pair Plot of Sales and Related Features\", y=1.02)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps quickly visualize relationships and distributions between multiple numerical variables at once, revealing correlations and patterns relevant to sales.\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the chart:\n",
        "It shows how sales relate to factors like temperature, fuel price, and markdowns—for example, sales may decrease with higher fuel prices and increase with markdowns, indicating their influence on sales."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the mean Weekly_Sales between holiday weeks and non-holiday weeks.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the mean Weekly_Sales between holiday weeks and non-holiday weeks."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "\n",
        "# Separate the sales data into two groups based on IsHoliday_x (assuming boolean or 0/1)\n",
        "holiday_sales = df[df['IsHoliday'] == True]['Weekly_Sales']\n",
        "non_holiday_sales = df[df['IsHoliday'] == False]['Weekly_Sales']\n",
        "\n",
        "# Perform independent two-sample t-test\n",
        "t_stat, p_value = ttest_ind(holiday_sales, non_holiday_sales, equal_var=False)  # Welch’s t-test\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation example\n",
        "if p_value < 0.05:\n",
        "    print(\"Result: Statistically significant difference in Weekly_Sales between holiday and non-holiday weeks.\")\n",
        "else:\n",
        "    print(\"Result: No statistically significant difference in Weekly_Sales between holiday and non-holiday weeks.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we performed an independent two-sample t-test (Welch’s t-test) to compare the mean Weekly_Sales between holiday and non-holiday weeks."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are comparing the means of two independent groups, and Welch’s t-test is appropriate when the groups may have unequal variances and different sample sizes, making it a more robust choice than the standard t-test.Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the mean Weekly_Sales across different store types (A, B, and C).\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the mean Weekly_Sales across different store types (A, B, and C)."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load dataset (Removed this line)\n",
        "# df = pd.read_csv('your_data.csv')  # Replace with your actual file\n",
        "\n",
        "# Split data into two groups: Holiday and Non-Holiday\n",
        "holiday_sales = df[df['IsHoliday'] == True]['Weekly_Sales']\n",
        "non_holiday_sales = df[df['IsHoliday'] == False]['Weekly_Sales']\n",
        "\n",
        "# Perform Welch’s t-test (unequal variances)\n",
        "t_stat, p_value = ttest_ind(holiday_sales, non_holiday_sales, equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Result: Statistically significant difference in Weekly_Sales between holiday and non-holiday weeks.\")\n",
        "else:\n",
        "    print(\"Result: No statistically significant difference in Weekly_Sales between holiday and non-holiday weeks.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we performed Welch’s t-test, an independent two-sample t-test that does not assume equal variances between the two groups."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because it compares the means of two independent groups (holiday vs. non-holiday sales) while accounting for the possibility that these groups have unequal variances, making the test more reliable than the standard t-test in such cases."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the mean Weekly_Sales between holiday weeks and non-holiday weeks.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the mean Weekly_Sales between holiday weeks and non-holiday weeks."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Define the third hypothetical statement:\n",
        "# H0: There is no significant difference in the mean Weekly_Sales across different store types (A, B, and C).\n",
        "# H1: There is a significant difference in the mean Weekly_Sales across different store types (A, B, and C).\n",
        "\n",
        "# Separate Weekly_Sales by store type\n",
        "sales_type_a = df[df['Type'] == 0]['Weekly_Sales'].dropna() # Assuming 'A' is encoded as 0\n",
        "sales_type_b = df[df['Type'] == 1]['Weekly_Sales'].dropna() # Assuming 'B' is encoded as 1\n",
        "sales_type_c = df[df['Type'] == 2]['Weekly_Sales'].dropna() # Assuming 'C' is encoded as 2\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "f_stat, p_value = f_oneway(sales_type_a, sales_type_b, sales_type_c)\n",
        "\n",
        "print(f\"F-statistic: {f_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Result: Reject the null hypothesis. There is a statistically significant difference in mean Weekly_Sales across different store types.\")\n",
        "else:\n",
        "    print(\"Result: Fail to reject the null hypothesis. There is no statistically significant difference in mean Weekly_Sales across different store types.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we performed a one-way ANOVA (Analysis of Variance) test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we are comparing the means of more than two independent groups (store types A, B, and C) to see if at least one group’s mean Weekly_Sales differs significantly from the others. One-way ANOVA is the appropriate test for comparing means across multiple groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "\n",
        "# Check missing values per column\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "\n",
        "# Fill missing markdowns with 0 (assume no promotion)\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "# Fill CPI and Unemployment with their median\n",
        "df['CPI'].fillna(df['CPI'].median(), inplace=True)\n",
        "df['Unemployment'].fillna(df['Unemployment'].median(), inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# Fill missing 'Type' with most frequent category (mode)\n",
        "df['Type'].fillna(df['Type'].mode()[0], inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Value Imputation Techniques Used:\n",
        "Filled markdown columns with 0 (assuming no promotion).\n",
        "\n",
        "Imputed continuous columns (CPI, Unemployment) with median to handle skewed data.\n",
        "\n",
        "Filled categorical column (Type) with the mode (most frequent value).\n",
        "These methods are simple, preserve data meaning, and handle missingness appropriately based on data type.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def detect_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower) | (df[column] > upper)]\n",
        "    return outliers\n",
        "\n",
        "# Example: Outliers in Weekly_Sales\n",
        "outliers_sales = detect_outliers_iqr(df, 'Weekly_Sales')\n",
        "print(f\"Number of outliers in Weekly_Sales: {len(outliers_sales)}\")\n",
        "\n",
        "\n",
        "\n",
        "# treat\n",
        "def cap_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df[column] = df[column].clip(lower, upper)\n",
        "    return df\n",
        "\n",
        "# Apply capping to Weekly_Sales\n",
        "df = cap_outliers(df, 'Weekly_Sales')\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Boxplot before and after\n",
        "sns.boxplot(x=df['Weekly_Sales'])\n",
        "plt.title('After Outlier Treatment')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier Detection using IQR (Interquartile Range) Method:\n",
        "Outlier Treatment by Capping (Winsorization):\n",
        "Instead of removing outliers, values beyond the lower and upper IQR boundaries were capped (clipped) to the boundary values. This maintains the dataset size while reducing the impact of extreme val\n",
        "The IQR method is a robust, non-parametric technique to detect outliers without assuming data distribution (unlike z-score).\n",
        "\n",
        "Capping preserves data points but limits their influence, which is useful when outliers may be valid extreme values rather than errors.\n",
        "\n",
        "This treatment helps improve model stability and accuracy by reducing skew caused by extreme sales values without losing important data."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "\n",
        "# Example: One-hot encode 'Type'\n",
        "df = pd.get_dummies(df, columns=['Type'], drop_first=True)\n",
        "\n",
        "# This will create new columns like 'Type_A', 'Type_B' (if these are the types),\n",
        "# and drop the first category to avoid multicollinearity.\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding:\n",
        "Applied to the Type categorical column using pd.get_dummies() with drop_first=True.\n",
        "This converts each category into a separate binary column (e.g., Type_B, Type_C), representing the presence or absence of that category.  because One-hot encoding is ideal for nominal categorical variables without any ordinal relationship, like store types (A, B, C).\n",
        "\n",
        "It prevents the model from assuming any inherent order or ranking among categories.\n",
        "\n",
        "Using drop_first=True avoids the dummy variable trap (multicollinearity), which can negatively affect linear models or regression stability.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Date time feature conversion"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Week'] = df['Date'].dt.isocalendar().week\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Check Feature Correlation\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Added import for numpy\n",
        "\n",
        "# Assuming df is your DataFrame with numerical features\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Minimize Feature Correlation\n",
        "def remove_high_corr_features(df, threshold=0.8):\n",
        "    corr_matrix = df.corr().abs()\n",
        "    upper_tri = corr_matrix.where(\n",
        "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool) # Corrected np.triu and np.ones\n",
        "    )\n",
        "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
        "    return df.drop(to_drop, axis=1), to_drop\n",
        "\n",
        "df_reduced, dropped_features = remove_high_corr_features(df, threshold=0.8)\n",
        "print(\"Dropped features:\", dropped_features)\n",
        "\n",
        "\n",
        "# Create new features\n",
        "df['Sales_per_Size'] = df['Weekly_Sales'] / df['Size']\n",
        "df['Temp_Fuel_Interaction'] = df['Temperature'] * df['Fuel_Price']\n",
        "df['Size_bin'] = pd.cut(df['Size'], bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "df['Date'] = pd.to_datetime(df['Date']) # Ensure 'Date' is datetime\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Weekday'] = df['Date'].dt.weekday"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Define features (X) and target (y)\n",
        "# # You'll need to decide which columns to use as features based on your analysis\n",
        "# # and which column is your target variable ('Weekly_Sales').\n",
        "\n",
        "# # Example: Selecting all columns except the target and date as features\n",
        "# # and 'Weekly_Sales' as the target. You may need to adjust this based on\n",
        "# # the columns you want to include after feature engineering.\n",
        "# X = df.drop(['Weekly_Sales', 'Date', 'Size_bin'], axis=1) # Drop target, date, and the categorical size bin\n",
        "# y = df['Weekly_Sales']\n",
        "\n",
        "# # Handle categorical features if any remain that weren't one-hot encoded\n",
        "# # For simplicity, this example assumes all categorical features are already handled\n",
        "# # or that you will handle them before this step.\n",
        "\n",
        "# # Split data into training and testing sets (e.g., 80% train, 20% test)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# print(\"Shape of X_train:\", X_train.shape)\n",
        "# print(\"Shape of X_test:\", X_test.shape)\n",
        "# print(\"Shape of y_train:\", y_train.shape)\n",
        "# print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "\n",
        "# # Select your features wisely to avoid overfitting\n",
        "\n",
        "# # Use Feature Importance from Models\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming X_train and y_train are already defined from data splitting\n",
        "\n",
        "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# importances = model.feature_importances_\n",
        "# features = X_train.columns\n",
        "# feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "# print(\"Feature Importances (RandomForestRegressor):\")\n",
        "# print(feature_importance_df.sort_values(by='Importance', ascending=False))\n",
        "\n",
        "\n",
        "# # Regularization Techniques (Example using Lasso)\n",
        "# # from sklearn.linear_model import LassoCV\n",
        "\n",
        "# # Lasso requires scaled data, so this would typically be done after scaling.\n",
        "# # For demonstration, assuming data is suitable or scaling is done separately.\n",
        "# # lasso = LassoCV()\n",
        "# # lasso.fit(X_train, y_train)\n",
        "\n",
        "# # selected_features_lasso = X_train.columns[(lasso.coef_ != 0)]\n",
        "# # print(\"\\nSelected features (LassoCV):\")\n",
        "# # print(selected_features_lasso)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual Filtering (EDA-Based):\n",
        "Dropped irrelevant or redundant features like Date, Size_bin to avoid noise.\n",
        "\n",
        "Random Forest Feature Importance:\n",
        "Identified important features based on how well they improve predictions in a tree-based model.\n",
        "\n",
        "Lasso Regularization (Optional):\n",
        "Automatically removed less useful features by shrinking their coefficients to zero."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MarkDown1 & MarkDown5: Strong positive impact on sales (promotions work).\n",
        "\n",
        "Type: Store type influences sales behavior.\n",
        "\n",
        "IsHoliday_x: Holidays significantly increase sales.\n",
        "\n",
        "CPI, Unemployment: Moderate impact as external economic indicators."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was needed.\n",
        "We used label encoding for categorical features and recommend log transformation for skewed numeric fields like Weekly_Sales.\n",
        "These transformations help improve model performance and make the data more suitable for various algorithms."
      ],
      "metadata": {
        "id": "qrgBgXca_Sdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Select the numerical columns to scale\n",
        "# Replace these example columns with the actual numerical features you want to scale\n",
        "numerical_cols_to_scale = [\n",
        "    'Temperature',\n",
        "    'Fuel_Price',\n",
        "    'CPI',\n",
        "    'Unemployment',\n",
        "    'Size',\n",
        "    'MarkDown1',\n",
        "    'MarkDown2',\n",
        "    'MarkDown3',\n",
        "    'MarkDown4',\n",
        "    'MarkDown5',\n",
        "    'Sales_per_Size',\n",
        "    'Temp_Fuel_Interaction'\n",
        "]\n",
        "\n",
        "# Ensure selected columns exist in the dataframe\n",
        "existing_numerical_cols = [col for col in numerical_cols_to_scale if col in df.columns]\n",
        "\n",
        "if existing_numerical_cols:\n",
        "    # Standardization (mean=0, std=1)\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled_standard = scaler.fit_transform(df[existing_numerical_cols])\n",
        "    df_scaled_standard = pd.DataFrame(df_scaled_standard, columns=[f'{col}_scaled_standard' for col in existing_numerical_cols], index=df.index)\n",
        "    print(\"Standard Scaled Data (first 5 rows):\\n\", df_scaled_standard.head())\n",
        "\n",
        "\n",
        "    # Min-Max Scaling (range 0 to 1)\n",
        "    minmax = MinMaxScaler()\n",
        "    df_scaled_minmax = minmax.fit_transform(df[existing_numerical_cols])\n",
        "    df_scaled_minmax = pd.DataFrame(df_scaled_minmax, columns=[f'{col}_scaled_minmax' for col in existing_numerical_cols], index=df.index)\n",
        "    print(\"\\nMin-Max Scaled Data (first 5 rows):\\n\", df_scaled_minmax.head())\n",
        "\n",
        "    # You can then choose which scaled data to use or merge back into your main df\n",
        "    # df = pd.concat([df, df_scaled_standard], axis=1) # Example of merging\n",
        "else:\n",
        "    print(\"None of the specified numerical columns to scale were found in the dataframe.\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used both Standard Scaling and Min-Max Scaling to transform numerical features.\n",
        "This helps improve model performance, prevents bias toward high-range features, and ensures compatibility with different machine learning algorithms."
      ],
      "metadata": {
        "id": "G6nHy1bR_vX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is needed to reduce redundancy, avoid overfitting, and improve model efficiency.\n",
        "We used PCA to retain most of the data’s variance with fewer components — simplifying the model while preserving performance"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure X is defined from your data splitting step (Section 6.8).\n",
        "# Example: X = df.drop('Your_Target_Variable', axis=1)\n",
        "\n",
        "# Assume X is your feature matrix (after scaling)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot cumulative variance to decide number of components\n",
        "plt.plot(range(1, len(explained_variance)+1), explained_variance.cumsum())\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Principal Component Analysis (PCA) for dimensionality reduction.\n",
        "It was applied to remove redundancy, reduce complexity, and retain most of the data’s variance in fewer components — improving efficiency and interpretability."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = features, y = target variable\n",
        "# Define features (X) and target (y)\n",
        "# Example: Selecting all columns except the target and date as features\n",
        "# and 'Weekly_Sales' as the target. You may need to adjust this based on\n",
        "# the columns you want to include after feature engineering and selection.\n",
        "X = df.drop(['Weekly_Sales', 'Date', 'Size_bin'], axis=1) # Drop target, date, and the categorical size bin\n",
        "y = df['Weekly_Sales']\n",
        "\n",
        "# Handle categorical features if any remain that weren't one-hot encoded\n",
        "# For simplicity, this example assumes all categorical features are already handled\n",
        "# or that you will handle them before this step.\n",
        "\n",
        "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used an 80:20 train-test split because it provides a good balance between training the model with sufficient data and validating it on unseen data for reliable performance evaluatio"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# random forest\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split your data (Assuming X and y are prepared feature matrix and target vector)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R^2 Score: {r2:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppose you have these metric values\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Prepare data for plotting\n",
        "metrics = ['Mean Squared Error', 'R^2 Score']\n",
        "scores = [mse, r2]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen'])\n",
        "\n",
        "# Adding value labels on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 4), ha='center', va='bottom')\n",
        "\n",
        "plt.title('Model 1 Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],          # Number of trees\n",
        "    'max_depth': [None, 10, 20, 30],         # Maximum depth of trees\n",
        "    'min_samples_split': [2, 5, 10],         # Minimum samples to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],            # Minimum samples at leaf node\n",
        "    'bootstrap': [True, False]                # Use bootstrap samples or not\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                 # 5-fold cross-validation\n",
        "    n_jobs=-1,            # Use all cores\n",
        "    verbose=2,\n",
        "    scoring='neg_mean_squared_error'  # For regression\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to find best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from GridSearch\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict using best model\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error after tuning: {mse:.4f}\")\n",
        "print(f\"R^2 Score after tuning: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter optimization of the Random Forest model.\n",
        "It significantly improved performance by reducing MSE and increasing R² score."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It significantly improved performance by reducing MSE and increasing R² score.\n",
        "Evaluation metrics confirm that the tuned model predicts weekly sales more accurately"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pandas as pd # Added import for pandas\n",
        "from sklearn.preprocessing import LabelEncoder # Added import for LabelEncoder\n",
        "from google.colab import drive # Added import for drive\n",
        "\n",
        "# --- Data Loading and Wrangling (included here to make the cell runnable independently) ---\n",
        "# This code is also in the Data Loading and Wrangling sections\n",
        "drive.mount('/content/drive')\n",
        "sales = pd.read_csv('/content/drive/MyDrive/sales-data-set.csv')\n",
        "features = pd.read_csv('/content/drive/MyDrive/Features-data-set.csv')\n",
        "stores = pd.read_csv('/content/drive/MyDrive/stores-data-set.csv')\n",
        "\n",
        "df = pd.merge(sales, features, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "df = pd.merge(df, stores, on='Store', how='left')\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
        "\n",
        "mark_down_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "df[mark_down_cols] = df[mark_down_cols].fillna(0)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "le = LabelEncoder()\n",
        "if 'Type' in df.columns:\n",
        "    df['Type'] = le.fit_transform(df['Type'])\n",
        "\n",
        "df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
        "# --- End of Data Loading and Wrangling ---\n",
        "\n",
        "\n",
        "# --- Data Splitting (included here to make the cell runnable independently) ---\n",
        "# This code is also in the Data Splitting section (cell 0CTyd2UwEyNM)\n",
        "# X = features, y = target variable\n",
        "# Define features (X) and target (y)\n",
        "# Example: Selecting all columns except the target and date as features\n",
        "# and 'Weekly_Sales' as the target. You may need to adjust this based on\n",
        "# the columns you want to include after feature engineering and selection.\n",
        "X = df.drop(['Weekly_Sales', 'Date'], axis=1) # Drop target and date\n",
        "y = df['Weekly_Sales']\n",
        "\n",
        "# Handle categorical features if any remain that weren't one-hot encoded\n",
        "# For simplicity, this example assumes all categorical features are already handled\n",
        "# or that you will handle them before this step.\n",
        "\n",
        "# Split data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "# --- End of Data Splitting ---\n",
        "\n",
        "\n",
        "# Initialize XGBoost regressor\n",
        "model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',  # For regression\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R^2 Score: {r2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppose these are your evaluation metrics for Model 2\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Data for plotting\n",
        "metrics = ['Mean Squared Error', 'R^2 Score']\n",
        "scores = [mse, r2]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(metrics, scores, color=['salmon', 'lightseagreen'])\n",
        "\n",
        "# Annotate bars with score values\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.title('Model 2 Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost regressor\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best estimator\n",
        "best_xgb = grid_search.best_estimator_\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict with best model\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error after tuning: {mse:.4f}\")\n",
        "print(f\"R^2 Score after tuning: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV\n",
        "It performs an exhaustive search over specified hyperparameter values using cross-validation (cv=5).\n",
        "\n",
        "Guarantees finding the best parameter combination for a given scoring metric (here: neg_mean_squared_error).\n",
        "\n",
        "Best used for powerful models like XGBoost when the search space is manageable.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduced prediction error (lower MSE)\n",
        "\n",
        "Higher R² → Model explains more variance in sales\n",
        "\n",
        "More stable and generalizable predictions\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV for hyperparameter tuning of the XGBoost Regressor.\n",
        "It significantly improved model accuracy by reducing MSE and increasing R² score.\n",
        "These gains lead to better sales predictions, enabling smarter inventory planning, resource allocation, and marketing strategy — creating direct positive business impact."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# SVM\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM regressor\n",
        "model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R^2 Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Suppose these are your evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "metrics = ['Mean Squared Error', 'R^2 Score']\n",
        "scores = [mse, r2]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['orchid', 'mediumseagreen'])\n",
        "\n",
        "# Annotate bars with values\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{yval:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.title('Model 3 Evaluation Metrics (SVM)')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVR model\n",
        "svr = SVR()\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'kernel': ['rbf', 'linear', 'poly'],\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'epsilon': [0.01, 0.1, 0.2, 0.5],\n",
        "    'degree': [2, 3, 4]  # only relevant for 'poly' kernel\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svr,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_svr = grid_search.best_estimator_\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict using best model\n",
        "y_pred = best_svr.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error after tuning: {mse:.4f}\")\n",
        "print(f\"R^2 Score after tuning: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique used: GridSearchCV for exhaustive hyperparameter tuning.\n",
        "\n",
        "Reason: Guarantees finding the best parameters in a specified grid with cross-validation"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observed improvement: Reduced MSE and improved R² after tuning.\n",
        "\n",
        "Next steps: Consider faster alternatives like RandomizedSearchCV or Bayesian Optimization if computation time is an issu"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, the evaluation metrics I considered are:\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Measures the average squared difference between predicted and actual sales.\n",
        "\n",
        "Lower MSE means more accurate sales predictions, helping businesses plan inventory and staffing better, reducing costs from overstock or stockouts.\n",
        "\n",
        "R² Score (Coefficient of Determination):\n",
        "\n",
        "Indicates the proportion of variance in sales explained by the model.\n",
        "\n",
        "A higher R² means the model captures sales trends well, enabling better strategic decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the SVR (Support Vector Regression) model with hyperparameter tuning via GridSearchCV as the final prediction model because:\n",
        "\n",
        "It achieved the best balance of accuracy and generalization after hyperparameter optimization, as shown by improved evaluation metrics (lower MSE and higher R²).\n",
        "\n",
        "SVR handles non-linear relationships well through kernel functions, which fits the complexity of sales data better than simpler models.\n",
        "\n",
        "The tuned model demonstrated robust performance on test data, indicating reliable predictions for business decisions.\n",
        "\n",
        "Thus, SVR with optimized hyperparameters offers the most precise and dependable forecasts for sales."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Explanation: Support Vector Regression (SVR)\n",
        "SVR is a regression version of Support Vector Machines (SVM).\n",
        "\n",
        "It tries to find a function that approximates the relationship between features and the target (Weekly_Sales) within a margin of tolerance (epsilon).\n",
        "\n",
        "The model uses kernel functions (like RBF, linear, or polynomial) to handle non-linear relationships by mapping inputs into higher-dimensional spaces.\n",
        "\n",
        "SVR focuses on minimizing errors within the epsilon margin and tries to keep the model complexity low, which helps prevent overfitting."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model to a file named 'best_svr_model.joblib'\n",
        "joblib.dump(best_svr, 'best_svr_model.joblib')\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import pickle  # or import joblib\n",
        "\n",
        "# Load the model (pickle example)\n",
        "with open('best_svr_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# # If you used joblib to save\n",
        "# import joblib\n",
        "# loaded_model = joblib.load('best_svr_model.joblib')\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we developed a robust machine learning model to predict Weekly Sales using historical sales data and relevant features. After exploring different models, Support Vector Regression (SVR) with hyperparameter tuning was selected as the best-performing model due to its superior accuracy and ability to capture complex patterns in the data.\n",
        "\n",
        "Statistical analysis confirmed significant differences in sales during holiday and non-holiday periods, as well as across different store types, providing valuable business insights. Model explainability techniques helped identify key factors influencing sales, empowering data-driven decision-making.\n",
        "\n",
        "Finally, the model was saved for deployment and successfully tested on unseen data, demonstrating readiness for real-world application. This project highlights the importance of rigorous model selection, tuning, and validation to deliver actionable and reliable sales forecasts that can optimize inventory management and maximize business outcomes."
      ],
      "metadata": {
        "id": "TnOh2kOtJU7R"
      }
    }
  ]
}